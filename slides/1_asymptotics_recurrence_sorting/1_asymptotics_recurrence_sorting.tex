%GiG
\documentclass{beamer} 
\usetheme{Copenhagen}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{headline}{}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{hyperref}
\definecolor{azure}{rgb}{0.0, 0.5, 1.0}
%\newcommand{\tblue}[1]{\textcolor{blue}{#1}}
\newcommand{\tblue}[1]{{\Large {\textcolor{azure}{#1}}}}
\newcommand{\hred}[1]{{\textcolor{red}{#1}}}

\title[Saravanan Thirumuruganathan] 
{Lecture 1: Asymptotics, Recurrences, Elementary Sorting}

\author[CSE 5311] 
{Instructor: Saravanan Thirumuruganathan}

\date[] 

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%\begin{frame}{Outline}
%  \tableofcontents
%  % You might wish to add the option [pausesections]
%\end{frame}

\section{Outline}

\begin{frame}
\frametitle {Outline}
\begin{enumerate}
\item Introduction to Asymptotic Analysis
\begin{itemize}
\item Rate of growth of functions
\item Comparing and bounding functions: $O, \Theta, \Omega$
\item Specifying running time through recurrences
\item Solving recurrences
\end{itemize}
\item Elementary Sorting Algorithms
\begin{itemize}
\item Bubble, Insertion and Selection sort
\item Stability of sorting algorithms
\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{In-Class Quizzes}
\begin{itemize}
\item {\Large {\bf URL:}} {\LARGE \bf \url{http://m.socrative.com/}} 
\item {\Large {\bf Room Name:} {\LARGE \bf 4f2bb99e}}
\end{itemize}
\end{frame}

\section{Analyzing Algorithms}
\begin{frame}{Analyzing Algorithms}

\tblue{Time Complexity:}
\begin{itemize}
\item Quantifies amount of \textbf{\textit{time}} an algorithm needs to complete as a {\em function} of input size
\end{itemize}
~\\
\tblue{Space Complexity:}
\begin{itemize}
\item Quantifies amount of \textbf{\textit{space}} an algorithm needs to complete as a {\em function} of input size
\end{itemize}
~\\
Function: Input size Vs \{Time, Space\}
\end{frame}

\begin{frame}{Analyzing Algorithms}

\tblue{Best Case Complexity:}
\begin{itemize}
\item of an algorithm is the {\bf function} that determines the {\bf minimum} number of steps taken on any problem instance of size $n$
\end{itemize}
\tblue{Worst Case Complexity:}
\begin{itemize}
\item $\ldots$ {\bf maximum} $\ldots$
\end{itemize}
\tblue{Average Case Complexity:}
\begin{itemize}
\item $\ldots$ {\bf average} $\ldots$
\end{itemize}
~\\
Function: Input size Vs \{Time, Space\}
\end{frame}

\begin{frame}{Rate of Growth of Functions}

\tblue{Growth Function $T(n)$}
\begin{itemize}
\item Input is positive integers $n=1,2,3, \ldots$
\item Asymptotically positive (returns positive numbers for large $n$)
\item How does $T(n)$ grow when $n$ grows?
\item $n$ is size of input
\item $T(n)$ is the amount of time it takes for an algorithm to solve some problem
\end{itemize}
\end{frame}


\begin{frame}{Rate of Growth of Functions}
\begin{center}
    \includegraphics[scale=0.3]{rateOfGrowth.jpg}
\end{center}
\end{frame}

\begin{frame}{Quiz!}

\tblue{Question:}
\begin{itemize}
\item You have a machine that can do million operations per second.
\item Your algorithm requires $n^2$ steps
\item Suppose size of input is 1 million
\item How long does the algorithm takes for this input?
\end{itemize}
\end{frame}


\begin{frame}{Quiz!}

\tblue{Answer:}
\begin{itemize}
\item Algorithm will take $(1M)^2$ operations
\item Machine can do $1M$ operations per second
\item Running time = $\frac{(1M)^2}{1M}$ = $1M$ seconds
\item $1M$ seconds = $\frac{1M}{60*60*24}$ = Approximately $12$ days
\end{itemize}
\end{frame}


\begin{frame}{Why does it matter?}
Running time of different algorithms for various input sizes
\begin{center}
    \includegraphics[scale=0.35]{fnVsTime.png}\footnote{Table 2.1 from K\&T Algorithm Design. Very long means it takes more than $10^{25}$ years.}
\end{center}
\end{frame}

\begin{frame}{Why does it matter?}
\begin{itemize}
\item The ``Big Data'' era
\item Can Google/Facebook/$\ldots$ use it?
\end{itemize}
\end{frame}


\begin{frame}{Functions in Real World}
This is how functions look in the real world!\footnote{Skiena Lecture notes}
\begin{center}
    \includegraphics[scale=0.4]{realWorldFunctions.png}
\end{center}
\end{frame}


\begin{frame}{Solution: Analyze Asymptotic Behavior}
\begin{itemize}
\item Analyze the asymptotic behavior of algorithms
\item What happens to $f(n)$ when $n \to \infty$?
\end{itemize}
\begin{center}
\begin{table}[h]
\begin{tabular}{|c|c|c|}
\hline
       & $T(n)=1000n$ & $T(n)=n^2$       \\ \hline
$n=10$   & $10K$          & $100$        \\ \hline
$n=100$  & $100K$         & $10K$        \\ \hline
$n=1000$ & $1M$           & $1M$         \\ \hline
$n=10K$  & $10M$          & $100M$       \\ \hline
$n=100K$  & $100M$          & $10B$       \\ \hline
\end{tabular}
\end{table}
\end{center}
\end{frame}

\begin{frame}{Solution: Bound the functions}
\begin{itemize}
\item Identify known functions (such as $n, n^2, n^3, 2^n, \ldots$) that can ``bound'' $T(n)$
\item How to bound? - asymptotic {\bf upper, lower} and {\bf tight} bounds
\item Find a function $f(n)$ such that $T(n)$ is {\bf proportional} to $f(n)$
\item Why proportional (as against {\bf equal})?
\item Ignore aspects such as programming language, programmer capability, compiler optimization, machine specification etc
\end{itemize}
\begin{center}
    \includegraphics[scale=0.4]{realWorldFunctions.png}
\end{center}
\end{frame}


\begin{frame}{$O, \Omega, \Theta$\footnote{CLRS book}}
\begin{center}
    \includegraphics[scale=0.32]{allThreeBoundingFunctions.png}
\end{center}
\end{frame}

\begin{frame}{Big-O Notation}

\tblue{Upper bounds:} $T(n)$ is $O(f(n))$ if there exists {\bf constants} $c>0$ and $n_0 \geq 0$ such that
$T(n) \leq c \cdot f(n)$ for all $n \geq n_0$
\begin{center}
    \includegraphics[scale=0.4]{bigOh.png}\footnote{From K\&T: Algorithm Design}
\end{center}
\end{frame}



\begin{frame}{Big-O Notation}

\tblue{Upper bounds:} $T(n)$ is $O(f(n))$ if there exists {\bf constants} $c>0$ and $n_0 \geq 0$ such that
$T(n) \leq c \cdot f(n)$ for all $n \geq n_0$

~\\
\textbf{Example:} $T(n)$ = $32n^2+17n+1$.  Is $T(n)$ in $O(n^2)$? 
\begin{itemize}
\item Yes! Use $c=50$, $n_0=1$
\item Simple Proof:
\end{itemize}
\begin{align*}
T(n) &\leq 32n^2+17n+1 \\
&\leq 32n^2+17n^2+1n^2 \\
&\leq 50n^2\\
&\leq cn^2\\
c=50    &   \text{ and } n_0=1
\end{align*}
Note: Not necessary to find the smallest $c$ or $n_0$
\end{frame}


\begin{frame}{Quiz!}
\textbf{Example:} $T(n)$ = $32n^2-17n+1$.  Is $T(n)$ in $O(n^2)$? 
\end{frame}

\begin{frame}{Big-O Notation}
\textbf{Example:} $T(n)$ = $32n^2-17n+1$.  Is $T(n)$ in $O(n^2)$? 
\begin{itemize}
\item Yes! Use $c=50$, $n_0=1$
\item Simple Proof:
\end{itemize}
\begin{align*}
T(n) &\leq 32n^2-17n+1 \\
&\leq 32n^2\textcolor{red}{+}17n+1 \\
&\leq 32n^2+17n^2+1n^2 \\
&\leq 50n^2\\
&\leq cn^2\\
c=50    &   \text{ and } n_0=1
\end{align*}
\end{frame}


\begin{frame}{Quiz!}
\textbf{Example:} $T(n)$ = $32n^2-17n+1$.  Is $T(n)$ in $O(n^3)$? 
\end{frame}

\begin{frame}{Big-O Notation}
\textbf{Example:} $T(n)$ = $32n^2-17n+1$.  Is $T(n)$ in $O(n^3)$? 
\begin{itemize}
\item Yes! Use $c=50$, $n_0=1$
\item Simple Proof:
\end{itemize}
\begin{align*}
T(n) &\leq 32n^2-17n+1 \\
&\leq 32n^2\textcolor{red}{+}17n+1 \\
&\leq 32n^2+17n^2+1n^2 \\
&\leq 50n^2\\
&\leq 50n^3\\
&\leq cn^3\\
c=50    &   \text{ and } n_0=1
\end{align*}
\end{frame}



\begin{frame}{Quiz!}
\textbf{Example:} $T(n)$ = $32n^2+17n+1$.  Is $T(n)$ in $O(n)$? 
\end{frame}

\begin{frame}{Big-O Notation}
\textbf{Example:} $T(n)$ = $32n^2+17n+1$.  Is $T(n)$ in $O(n)$? 
\begin{itemize}
\item No!
\item Proof by contradiction
\end{itemize}
\begin{align*}
32n^2+17n+1 &\leq c \cdot n\\
32n+17+\frac{1}{n} &\leq c \\
32n &\leq c \text{ (ignore constants for now)} \\
n &\leq c \text{ (ignore constants for now)} \\
\end{align*}
This inequality does not hold for $n=c+1$!
\end{frame}



\begin{frame}{Set Theoretic Perspective}
\begin{itemize}
\item $O(f(n))$ is set of all functions $T(n)$ where there exist positive constants $c, n_0$ 
such that $0 \leq T(n) \leq c \cdot f(n)$ for all $n \geq n_0$
\item Example: $O(n^2)=$ \{ $n^2, \ldots, 32n^2+17n+1$, $32n^2-17n+1$, $\ldots, n, 2n, \ldots $\}
\item Notation: $T(n)=O(f(n))$ or $T(n) \in O(f(n))$
\end{itemize}
\end{frame}


\begin{frame}{Limit based Perspective}
\begin{itemize}
\item $T(n)$ is $O(f(n))$ if $\limsup\limits_{n \rightarrow \infty} \frac{T(n)}{f(n)} < \infty$ 
\item Example: $32n^2+17n+1$ is $O(n^2)$
\end{itemize}
\begin{align*}
\limsup\limits_{n \rightarrow \infty} \frac{T(n)}{f(n)} &= \frac{32n^2+17n+1}{n^2} \\
&= 32 + \frac{17}{n} + \frac{1}{n^2} \\
&= 32 < \infty
\end{align*}
\end{frame}


\begin{frame}{Big-Omega Notation}

\tblue{Lower bounds:} $T(n)$ is $\hred{\Omega}(f(n))$ if there exists {\bf constants} $c>0$ and $n_0 \geq 0$ such that
$T(n) \hred{\geq} c \cdot f(n)$ for all $n \geq n_0$
\begin{center}
    \includegraphics[scale=0.4]{bigOmega.png}\footnote{From K\&T: Algorithm Design}
\end{center}
\end{frame}


\begin{frame}{Big-Omega Notation}

\tblue{Lower bounds:} $T(n)$ is $\Omega(f(n))$ if there exists {\bf constants} $c>0$ and $n_0 \geq 0$ such that
$T(n) \geq c \cdot f(n)$ for all $n \geq n_0$

~\\
\textbf{Example:} $T(n)$ = $32n^2+17n+1$.  Is $T(n)$ in $\Omega(n^2)$? 
\begin{itemize}
\item Yes! Use $c=32$, $n_0=1$
\item Simple Proof:
\end{itemize}
\begin{align*}
T(n) &\geq 32n^2+17n+1 \\
&\geq 32n^2\\
&\geq cn^2\\
c=32    &   \text{ and } n_0=1
\end{align*}
\end{frame}


\begin{frame}{Big-Theta Notation}

\tblue{Tight bounds:} $T(n)$ is $\hred{\Theta}(f(n))$ if there exists {\bf constants} $\hred{c_1>0, c_2>0}$ and $n_0 \geq 0$ such that
\hred{$c_1 \cdot f(n) \leq T(n) \leq c_2 \cdot f(n)$} for all $n \geq n_0$
\begin{center}
    \includegraphics[scale=0.4]{bigTheta.png}\footnote{From K\&T: Algorithm Design}
\end{center}
\end{frame}


\begin{frame}{Big-Theta Notation}

\tblue{Tight bounds:} $T(n)$ is $\hred{\Theta}(f(n))$ if there exists {\bf constants} $\hred{c_1>0, c_2>0}$ and $n_0 \geq 0$ such that
\hred{$c_1 \cdot f(n) \leq T(n) \leq c_2 \cdot f(n)$} for all $n \geq n_0$

~\\
\textbf{Example:} $T(n)$ = $32n^2+17n+1$.  Is $T(n)$ in $\Omega(n^2)$? 
\begin{itemize}
\item Yes! Use $c_1=32$, $c_2=50$ and $n_0=1$
\item Combine proofs from before
\end{itemize}

~\\
{\bf Theorem:} For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if 
$f(n) = O(g(n))$ and $f(n)=\Omega(g(n))$ 
\end{frame}



\begin{frame}{Limit based Definitions}
Let $\limsup\limits_{n \rightarrow \infty} \frac{T(n)}{f(n)} = c$
\begin{itemize}
\item If $c < \infty$ then $T(n)$ is $O(f(n))$  (typically $c$ is zero)
\item If $c > 0$ then $T(n)$ is $\Theta(f(n))$  (also $O(f(n))$ and $\Omega(f(n))$)
\item If $c = \infty$ then $T(n)$ is $\Omega(f(n))$ 
\end{itemize}
\end{frame}

\begin{frame}{Some Big-O tips}
\begin{itemize}
\item Big-O is one of the most useful things you will learn in this class!
\item Big-O ignores {\em constant} factors through $c$ 
\begin{itemize}
\item Algorithm implemented in Python might need a larger $c$ than one implemented in C++
\end{itemize}
\item Big-O ignores small inputs through $n_0$ 
\begin{itemize}
\item Simply set a large value of $n_0$
\end{itemize}
\item Suppose $T(n)$ is $O(f(n))$. Typically, $T(n)$ is messy while $f(n)$ is simple 
\begin{itemize}
\item $T(n)=32n^2+17n+1$, $f(n)=n^2$
\end{itemize}
\item Big-O hides constant factors. Some times using an algorithm with worser Big-O might still be a good idea 
(e.g. sorting, finding medians)
\end{itemize}
\end{frame}

\begin{frame}{Survey of Running Times}
\begin{center}
    \begin{table}[h]
        \begin{tabular}{|l|l|p{2in}|}
            \hline
            {\bf Complexity} & {\bf Name} & {\bf Example} \\ \hline
            $O(1)$ & Constant time & Function that returns a constant (say 42)\\ \hline
            $O(\log n)$ & Logarithmic & Binary Search \\ \hline
            $O(n)$ & Linear & Finding Max of an array\\ \hline
            $O(n \log n)$ & Linearithmic & Sorting (for e.g. Mergesort) \\ \hline
            $O(n^2)$ & Quadratic & Selection sort \\ \hline
            $O(n^3)$ & Cubic & Floyd-Warshall \\ \hline
            $O(n^k)$ & Polynomial & Subset-sum with $k$ elements\\ \hline
            $O(2^n)$ & Exponential & Subset-sum with no cardinality constraints\\ \hline
        \end{tabular}
    \end{table}
\end{center}
\end{frame}


\begin{frame}{Dominance Rankings\footnote{Skiena lecture notes}}
\begin{itemize}
\item $n! \gg 2^n \gg n^3 \gg n^2 \gg n \log n \gg n \gg \log n \gg 1$
\item Exponential algorithms are useless even at $n >= 50$
\item Quadratic algorithms at around $n \geq 1M$
\item $O(n \log n)$ at around $n \geq 1B$
\end{itemize}
\end{frame}

\section{Recurrences}
\begin{frame}{Closer Look at $T(n)$}
\begin{itemize}
\item So far we assumed someone gave us $T(n)$ 
\item What is $n$? (Program Analysis)
\item How do we get $T(n)$? (Recurrences)
\end{itemize}
\end{frame}


\begin{frame}[fragile]{Program Analysis}
  \begin{columns}
    \begin{column}{0.5\textwidth}
\begin{verbatim}
for i=1 to n
{
    constant time
    operations
}
\end{verbatim}
    \end{column}
    \begin{column}{0.5\textwidth}
\begin{verbatim}
for i=1 to n
{
    for j=1 to n
    {
        constant time 
        operations 
    }
}
\end{verbatim}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Recurrences}
\begin{itemize}
\item Typically programs are lot more complex than that
\item Recurrences occur in recursion and divide and conquer paradigms
\item Specify running time as a function of $n$ and running time over inputs of smaller sizes
\item Examples:
\begin{itemize}
    \item fibonacci($n$) = fibonacci($n-1$) + fibonacci($n-2$)
    \item $T(n) = 2T(\frac{n}{2}) + n$
    \item $T(n) = T(n-1) + n$
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Solving Recurrences}
\begin{itemize}
\item Unrolling
\item Guess and prove by induction (aka Substitution)
\item Recursion tree
\item Master method
\end{itemize}
\end{frame}


\begin{frame}{Unrolling}
Let $T(n) = T(n-1) + n$. Base case: $T(1)=1$
\begin{align*}
T(n)    &= T(n-1) + n \\
        &= n + T(n-1) \\
        &= n + n-1 + T(n-2) \\
        &= n + n-1 + n-2 + T(n-3) \\
        &= n + n-1 + n-2 + n-3 + \ldots + 4 + 3 + 2 + 1 \\
        &= \frac{n(n+1)}{2} \\
        &= 0.5n^2 + 0.5n \\
        &= O(n^2)
\end{align*}
\end{frame}

\begin{frame}{Quiz!}
Solve $T(n) = 2T(n-1)$. Base case: $T(1)=1$ 
\end{frame}


\begin{frame}{Quiz!}
Solve $T(n) = 2T(n-1)$. Base case: $T(1)=1$ 
\begin{align*}
T(n)    &= 2T(n-1) \\
        &= 2( 2T(n-2) ) \\
        &= 2( 2( 2T(n-3))) \\
        &= 2^3 T(n-3) \\
        &= 2^i \ldots 2T(n-i) \\
        &= 2^n \\
        &= O(2^n)
\end{align*}
\end{frame}


\begin{frame}{Recursion Tree}
$T(n) = T(\frac{2n}{3}) + T(\frac{n}{3}) + cn$
\begin{center}
    \includegraphics[scale=0.4]{recursionTree.png}\footnote{From CLRS}
\end{center}
\end{frame}

\begin{frame}{Logarithms\footnote{Skiena Lectures}}
\begin{itemize}
\item Logarithm is an {\em inverse} exponential function
\item $b^x = n$ implies $x=\log_b n$
\item If $b=2$, logarithms reflect how many times we can double something until we get $n$ or halve something till we get $1$
\item Example: $2^4=16$, $\log_2 16 = \hred{\lg } 16 = 4$
\item Example: You need $\lg 256 = 8$ bits to represent $[0,255]$
\item Identities:
\begin{itemize}
    \item $\log_b (xy) = \log_b (x) + \log_b(y)$
    \item $\log_b a = \frac{\log_c a}{\log_c b}$
    \item $\log_b b = 1$ and $\log_b 1 = 0$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Master Method}
\begin{itemize}
    \item A ``black box'' method to solve recurrences that occur from Divide and Conquer algorithms
    \item Divide, Conquer and Combine steps 
    \item $T(n) = aT(\frac{n}{b}) + f(n)$
    \item Assumes that all sub-problems are of {\bf equal} size
    \item $a$ - number of sub-problems (equivalently, \#recursive calls)
    \item $b$ - the rate at which the problem shrinks
    \item Note: $a$ and $b$ must be constants  (it cannot be for e.g. $\sqrt{n}$ )
    \item $f(n)$ - complexity of the combine step 
\end{itemize}
\end{frame}

\begin{frame}{Master Method}

{\bf Master Theorem:}  Let $a \geq 1$, $b > 1$ and $d \geq 0$ be constants (for e.g. they cannot be $\sqrt{n}$).
Let $T(n)$ be defined on the non-negative integers by recurrence as 
                    $$T(n) = \hred{a} T(\frac{n}{\hred{b}}) + n^{\hred{d}}$$
Then $T(n)$ has the following asymptotic bounds:
\[
 T(n) =
  \begin{cases}
   O(n^d \log n) & \text{if } a = b^d \\
   O(n^d) & \text{if } a < b^d \\
   O(n^{log_b a}) & \text{if } a > b^d \\
  \end{cases}
\]
\end{frame}


\begin{frame}{Master Method - Examples \footnote{From Tim Roughgarden's notes}}
$T(n) = 2T(\frac{n}{2}) + n$
\begin{itemize}
\item $a=2$, $b=2$ and $d=1$ (as $n = n^1$)
\item $b^d = 2^1 = 2 = a$ 
\item By case 1, $T(n) = O(n^d \log n) = O(n \log n)$
\end{itemize}
\end{frame}


\begin{frame}{Master Method - Examples \footnote{From Tim Roughgarden's notes}}
$T(n) = 2T(\frac{n}{2}) + n^2$
\begin{itemize}
\item $a=2$, $b=2$ and $d=2$ 
\item $b^d = 2^2 = 4 > a$ 
\item By case 2, $T(n) = O(n^d) = O(n^2)$
\end{itemize}
\end{frame}

\begin{frame}{Master Method - Examples \footnote{From Tim Roughgarden's notes}}
$T(n) = 4T(\frac{n}{2}) + n$
\begin{itemize}
\item $a=4$, $b=2$ and $d=1$ (as $n = n^1$)
\item $b^d = 2^1 = 2 < a$ 
\item By case 3, $T(n) = O(n^{\log_b a}) = O(n^{\log_2 4}) = O(n^2)$
\end{itemize}
\end{frame}



\begin{frame}{Quiz!\footnote{From Wikipedia}}
Solve $T(n) = 8T(\frac{n}{2}) + 1000n^2$. Let's solve it step by step!
\end{frame}

\begin{frame}{Quiz!\footnote{From Wikipedia}}
Solve $T(n) = 8T(\frac{n}{2}) + 1000n^2$
\begin{itemize}
\item $a=8$, $b=2$ and $d=2$
\item $b^d = 2^2 < a$
\item Falls in Case 3 of Master Theorem
\item $O(n^{\log_b a}) = O(n^{\log_2 8}) = O(n^3)$
\end{itemize}
\end{frame}


\section{Elementary Sorting Algorithms}
\begin{frame}{Sorting Problem}
\begin{itemize}
\item {\bf Input:} A sequence of $n$ numbers $\langle a_1, a_2, \ldots, a_n \rangle$
\item {\bf Output:} A permutation (reordering) $\langle a_1', a_2', \ldots,  a_n' \rangle$ of the input sequence 
such that $a_1' \leq a_2' \leq \ldots \leq a_n'$
\item {\bf Example:} $\langle 4, 2, 1, 3, 5\rangle$ to $\langle 1, 2, 3, 4, 5\rangle$
\item Assume distinct values (doesn't affect correctness or analysis)
\end{itemize}
\end{frame}

\begin{frame}{Applications of Sorting\footnote{From Slides of Kevin Wayne}}
\begin{enumerate}
\item Direct applications
\begin{itemize}
    \item Sorting a list of names in dictionary
    \item Sorting search results based on Google's ranking algorithm
\end{itemize}
\item Problems made simpler after sorting
\begin{itemize}
    \item Finding median, frequency distribution
    \item Finding duplicates
    \item Binary search
    \item Closest pair of points
\end{itemize}
\item Non-obvious applications
\begin{itemize}
    \item Data compression (e.g. Huffman encoding)
    \item Computer Graphics (e.g Convex hulls)
    \item Many many more !
\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Sorting Algorithms}
\begin{itemize}
\item Comparison based sorting
\begin{itemize}
\item Time complexity measured in terms of comparisons
\item Elementary algorithms: Bubble, Selection and Insertion sort
\item Mergesort and Quicksort 
\end{itemize}
\item Non-comparison based sorting
\begin{itemize}
\item Bucket, Counting and Radix sort
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Facets of Sorting Algorithms}
\begin{itemize}
\item Best, Average and Worst case time complexity 
\item Worst case space complexity
\item {\bf In-Place:} Transforms the input data structure with only constant additional space
\item {\bf Stability:} The relative order of items with same key values. 
E.g. $\langle 100_1, 400, 100_2, 200\rangle \rightarrow \langle 100_1, 100_2, 200, 400 \rangle$
\item {\bf Adaptive:} Can it leverage the ``sortedness'' of input?
\end{itemize}
\end{frame}



\begin{frame}{Bubble Sort}
\begin{itemize}
\item {\bf Basic Idea:}
\begin{itemize}
    \item Compare adjacent elements
    \item Swap them if they are in wrong order
    \item Repeat till entire array is sorted
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Bubble Sort\footnote{\url{http://www.cs.miami.edu/~ogihara/csc220/slides/Ch08.pdf}}}
\begin{center}
    \includegraphics[scale=0.5]{bubbleSort.png}
\end{center}
\end{frame}



\begin{frame}[fragile]{Bubble Sort}

\tblue{Pseudocode:}
\begin{verbatim}
BubbleSort(A):
    for i = 1 to A.length - 1
        for j = A.length downto i+1 
            if A[j] < A[j-1]
                swap(A[j-1], A[j])
\end{verbatim}

\tblue{Loop Invariant:} First $i-1$ elements are in sorted order
\vspace{1cm}

\tblue{Better Implementation:} Count swaps within an iteration. Terminate if no swaps.
\end{frame}



\begin{frame}{Quiz!}

\tblue{Bubble Sort Properties:}

\begin{itemize}
\item Best case time complexity:  $O(n)$
\item Worst case time complexity: $O(n^2)$
\item Adaptive: Yes
\item In-Place: Yes
\item Stability: Yes
\end{itemize}
\end{frame}


\begin{frame}{Selection Sort}
\begin{itemize}
\item {\bf Basic Idea:}
\begin{itemize}
    \item Find smallest element and exchange it with A[1]
    \item Find second smallest element and exchange it with A[2]
    \item Repeat process till entire array is sorted
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Selection Sort\footnote{\url{http://www.cs.miami.edu/~ogihara/csc220/slides/Ch08.pdf}}}
\begin{center}
    \includegraphics[scale=0.48]{selectionSort.png}
\end{center}
\end{frame}




\begin{frame}[fragile]{Selection Sort}

\tblue{Pseudocode:}
\begin{verbatim}
SelectionSort(A):
    for i = 1 to A.length 
        k = i
        for j = i+1 to A.length 
            if A[j] < A[k]
                k = j
        swap(A[i], A[k])
\end{verbatim}

\tblue{Loop Invariant:} First $i-1$ elements are in sorted order
\end{frame}



\begin{frame}{Quiz!}

\tblue{Selection Sort Properties:}

\begin{itemize}
\item Best case time complexity:  $O(n^2)$
\item Worst case time complexity: $O(n^2)$
\item Adaptive: No 
\item In-Place: Yes
\item Stability: No (but can be made to one with some effort) 
\end{itemize}
\end{frame}




\begin{frame}{Insertion Sort}
\begin{itemize}
\item Best of the elementary sorting algorithms 
\item {\bf Basic Idea:}
\begin{itemize}
    \item Start with an empty sorted array
    \item Pick an element and insert into the right place
    \item Repeat till all elements are handled
\end{itemize}
\end{itemize}
\end{frame}



\begin{frame}{Insertion Sort\footnote{\url{http://www.cs.miami.edu/~ogihara/csc220/slides/Ch08.pdf}}}
\begin{center}
    \includegraphics[scale=0.5]{insertionSort.png}
\end{center}
\end{frame}




\begin{frame}[fragile]{Insertion Sort}

\tblue{Pseudocode:}
\begin{verbatim}
InsertionSort(A):
    for i = 2 to A.length
        key = A[i]
        j = i - 1
        while j > 0 and A[j] > key
            A[j+1] = A[j]
            j = j - 1
        A[j+1] = key
\end{verbatim}

\tblue{Loop Invariant:} At start of i-th iteration, subarray A[1..i-1] consists of elements originally in A[1..i-1] but in sorted order
\end{frame}


\begin{frame}{Quiz!}

\tblue{Insertion Sort Properties:}

\begin{itemize}
\item Best case time complexity:  $O(n)$
\item Worst case time complexity: $O(n^2)$
\item Adaptive: Yes
\item In-Place: Yes
\item Stability: Yes
\end{itemize}
\end{frame}

\begin{frame}{Quiz!}

Suppose you have an array with identical elements. Which sort would take the least time?
\end{frame}


\begin{frame}{Quiz!}

Suppose you have an array with identical elements. Which sort would take the least time?
\begin{itemize}
\item Insertion sort
\item Bubble sort
\end{itemize}
\end{frame}


\begin{frame}{Quiz!}

Suppose you have an array that is $k$-sorted - i.e. each element is at most $k$ away from its target position.
For example, $\langle 1 , 3, 0, 2 \rangle$ is a $2-$sorted array.
\end{frame}


\begin{frame}{Quiz!}

Suppose you have an array that is $k$-sorted - i.e. each element is at most $k$ away from its target position.
For example, $\langle 1 , 3, 0, 2 \rangle$ is a $2-$sorted array.
\begin{itemize}
\item Insertion sort
\end{itemize}
\end{frame}


\begin{frame}{Summary}

\tblue{Major Topics:}
\begin{itemize}
\item Asymptotics,  $O, \Omega, \Theta$, Recurrences, Master method
\item Sorting, facets of Sorting, elementary Sorting algorithms
\end{itemize}
\end{frame}


\end{document}

